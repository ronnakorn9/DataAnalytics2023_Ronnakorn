{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import data\n",
    "path = \"./data/raw/sem_eval_task6/\"\n",
    "save_path = \"./data/converted/sem_eval_task6/\"\n",
    "# filename = \"dev_set_task1.json\"\n",
    "filename = \"dev_set_task2.json\"\n",
    "# filename = \"training_set_task1.json\"\n",
    "# filename = \"training_set_task2.json\"\n",
    "\n",
    "text_only = True\n",
    "\n",
    "with open(path + filename, 'r', encoding=\"utf-8\") as input_file:\n",
    "    data = input_file.read()\n",
    "    structure = json.loads(data)\n",
    "\n",
    "len(structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = \"./data/raw/sem_eval_task6/valid_label.txt\"\n",
    "with open(label_path, 'r', encoding=\"utf-8\") as input_file:\n",
    "    categories = input_file.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "data_frame = pd.DataFrame(columns=['class'])\n",
    "categories_dict = {key : 0 for key in categories}\n",
    "for entry in structure:\n",
    "    if text_only:\n",
    "        doc = nlp(entry[\"text\"])\n",
    "        doc.cats = {category: 0 for category in categories}\n",
    "        \n",
    "        for label in entry[\"labels\"]:\n",
    "            \n",
    "            doc.cats[label] = 1\n",
    "            categories_dict[label] = categories_dict[label] + 1\n",
    "            data_frame.loc[len(data_frame.index)] = label\n",
    "        doc_list.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### spancat data\n",
    "data_frame = pd.DataFrame(columns=['class', 'size'])\n",
    "nlp = spacy.blank(\"en\")\n",
    "span_key = \"sc\"\n",
    "for entry in structure:\n",
    "    if text_only:\n",
    "        doc = nlp(entry[\"text\"])\n",
    "        span_list = []\n",
    "        for label in entry[\"labels\"]:\n",
    "            \n",
    "            tup = doc.char_span(label[\"start\"], label[\"end\"], label[\"technique\"])\n",
    "            span_list.append(tup)\n",
    "            data_frame.loc[len(data_frame.index)] = [label[\"technique\"], len(tup)]\n",
    "\n",
    "        doc.spans[span_key] = span_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data_frame['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, ax = plt.subplots(figsize=(15, 7))\n",
    "g=sns.barplot(data=data_frame.groupby('class').sum(), x='size', y='class', hue='class', legend=False)\n",
    "ax = g\n",
    "for c in ax.containers:\n",
    "    labels = [f'{v.get_width():.0f}' for v in c]\n",
    "    ax.bar_label(c, labels=labels, label_type='edge')\n",
    "# plt.xticks(rotation = 75)\n",
    "plt.title(\"Span Class distribution (total token)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "g=sns.histplot(data=data_frame, x =\"size\", legend=False)\n",
    "\n",
    "plt.title(\"Span size distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.sort_values('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "g=sns.catplot(data=data_frame.sort_values('class'), y=\"class\", kind=\"count\", hue='class', legend=False)\n",
    "g.fig.set_size_inches(65,5)\n",
    "ax = g.facet_axis(0, 0)\n",
    "for c in ax.containers:\n",
    "    labels = [f'{v.get_width()}' for v in c]\n",
    "    ax.bar_label(c, labels=labels, label_type='edge')\n",
    "plt.title(\"Span Class distribution (total span)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict\n",
    "sum(categories_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([token for token in doc_list[0]])\n",
    "[token.lemma_ for token in doc_list[0] if not token.is_stop and not token.is_punct and not token.text.isspace()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "g=sns.catplot(data=data_frame, y=\"class\", kind=\"count\", hue='class', legend=False)\n",
    "g.fig.set_size_inches(65,5)\n",
    "ax = g.facet_axis(0, 0)\n",
    "for c in ax.containers:\n",
    "    labels = [f'{v.get_width()}' for v in c]\n",
    "    ax.bar_label(c, labels=labels, label_type='edge')\n",
    "plt.title(\"Class distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_2 = pd.DataFrame(columns=[\"text\", \"word_count\", \"sentiment_direction\", \"sentiment_scale\", \"date\"])\n",
    "word_corpus = []\n",
    "token_corpus = []\n",
    "token_corpus_2 = []\n",
    "counter = Counter()\n",
    "for i, doc in enumerate(doc_list): \n",
    "    words = [token.text\n",
    "             for token in doc\n",
    "             if not token.is_stop and not token.is_punct and not token.text.isspace()]\n",
    "    counter.update(words)\n",
    "    lemma_list = [token.lemma_\n",
    "             for token in doc\n",
    "             if not token.is_stop and not token.is_punct and not token.text.isspace()]\n",
    "    vecs = [token.has_vector\n",
    "             for token in doc\n",
    "             if not token.is_stop and not token.is_punct and not token.text.isspace()]\n",
    "    pos_list = [token.pos_\n",
    "             for token in doc\n",
    "             if not token.is_stop and not token.is_punct and not token.text.isspace()]\n",
    "    \n",
    "    tag_list = [token.tag_\n",
    "             for token in doc\n",
    "             if not token.is_stop and not token.is_punct and not token.text.isspace()]\n",
    "    \n",
    "    vocab_list = [token.vocab\n",
    "             for token in doc\n",
    "             if not token.is_stop and not token.is_punct and not token.text.isspace()]\n",
    "    ent_list = [token.ent_type_\n",
    "             for token in doc\n",
    "             if not token.is_stop and not token.is_punct and not token.text.isspace()]\n",
    "    ### data frame\n",
    "\n",
    "    word_corpus.append(structure[i][\"text\"])\n",
    "    token_corpus.append(lemma_list)\n",
    "    token_corpus_2.append(words)\n",
    "counter.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x)\n",
    "X = vectorizer.fit_transform(token_corpus) ### use lemma\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.shape) # (688, 2784)\n",
    "names = vectorizer.get_feature_names_out()\n",
    "X ##### this is count matrix\n",
    "count_matrix = pd.DataFrame(X.toarray(), columns = names)\n",
    "### next, get the correlation matrix and plot the network\n",
    "print(len(count_matrix), \" rows\")\n",
    "count_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = count_matrix.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = corr.stack().reset_index()\n",
    "links.columns = ['var1', 'var2', 'value']\n",
    "print(\"all links shape: \", links.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(links.shape)\n",
    "print(links.loc[links['value'] > 0.1].shape)\n",
    "print(links.loc[links['value'] > 0.2].shape)\n",
    "print(links.loc[links['value'] > 0.5].shape)\n",
    "print(links.loc[links['value'] > 0.75].shape)\n",
    "print(links.loc[links['value'] > 0.9].shape)\n",
    "print(links.loc[links['value'] > 0.95].shape)\n",
    "print(links.loc[links['value'] > 0.99].shape)\n",
    "print(links.loc[links['value'] == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "links_filtered=links.loc[ (links['value'] > threshold) & (links['var1'] != links['var2']) ]\n",
    "print(\"links_filtered shape: \", links_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlation distribution\")\n",
    "sns.displot(links_filtered.loc[links_filtered['value'] != 1], x='value', bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making correlation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_list(count_matrix: pd.DataFrame, print_output=False):\n",
    "    '''\n",
    "        input: \n",
    "            count_matrix: dataframe from the \"vectorizing\" function\n",
    "            excel_index: boolean option indicating whether to transform row index into excel row index\n",
    "            print_output: boolean option indicating whether to print top k most distinct word\n",
    "        output: a dictionary with key = unique word, value = list of (row index + 2) of where that word shows up\n",
    "        ex. \"bless\": [35,47,49,87] means that word \"bless\" shows up on row 35,47,49,87\n",
    "        note: add 2 to chage it from index in dataframe to index in excel file\n",
    "    '''\n",
    "    k = 15\n",
    "    count_dict = {}\n",
    "    word_list = count_matrix.columns.values\n",
    "    for word in word_list:\n",
    "        count_dict[word] = (count_matrix.index[count_matrix[word] > 0]).tolist()\n",
    "    if print_output == True:\n",
    "        ### print which word is associated with what rows (in excel)\n",
    "        # print(json.dumps(count_dict, sort_keys=False, indent=4))\n",
    "\n",
    "        unqiue_count_dict = {k:len(v) for k, v in count_dict.items()}\n",
    "        df = pd.DataFrame.from_dict(unqiue_count_dict, orient='index')\n",
    "        df.columns = ['unique_count']\n",
    "        ax = df.nlargest(k, 'unique_count').plot.bar(title=f\"top {k} most frequent word\", rot=30) ### as in, if word \"we\" happens 10 times across 4 different post, thhe value will be '4'\n",
    "        for p in ax.patches:\n",
    "            ax.annotate(str(p.get_height()), xy=(p.get_x(), p.get_height()))\n",
    "        #plt.show()\n",
    "\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = get_word_list(count_matrix, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_as_size = False\n",
    "plot_type = 'spectral'\n",
    "\n",
    "print(\"pair_corr_plot: making network\")\n",
    "G = nx.from_pandas_edgelist(links_filtered, 'var1', 'var2', edge_attr='value')\n",
    "def assign_color(correlation):\n",
    "    if correlation <= 0:\n",
    "        return \"#ffa09b\"\n",
    "    elif correlation == 1:\n",
    "        return \"#00e541\"\n",
    "    else:\n",
    "        return \"#9eccb7\"\n",
    "\n",
    "def assign_thickness(correlation, benchmark_thickness=6, scaling_factor=2):\n",
    "    return benchmark_thickness * abs(correlation)**scaling_factor\n",
    "\n",
    "edge_color = []\n",
    "edge_width = []\n",
    "for key, value in nx.get_edge_attributes(G, 'value').items():\n",
    "    edge_color.append(assign_color(value))\n",
    "    edge_width.append(assign_thickness(value))\n",
    "node_size = []\n",
    "word_dict = get_word_list(count_matrix)\n",
    "print(\"pair_corr_plot: adjusting node size...\")\n",
    "if deg_as_size == True:\n",
    "    print(\"using degree as node size...\")\n",
    "for key, value in dict(G.degree).items():\n",
    "    \n",
    "    if deg_as_size == True:\n",
    "        ### value is the degree of the key (a word)\n",
    "        scaling_factor = 50\n",
    "        node_size.append(value * scaling_factor)\n",
    "    else:\n",
    "        if plot_type == 'spectral':\n",
    "            scaling_factor = 100\n",
    "            node_size.append(2*(len(word_dict[key])**2) * scaling_factor)\n",
    "        elif plot_type == 'circular':\n",
    "                scaling_factor = 8\n",
    "                node_size.append(2*(len(word_dict[key])**2) * scaling_factor)\n",
    "        else:\n",
    "            print(\"plot type is not recognized. received \", plot_type)\n",
    "            exit(-1)\n",
    "        \n",
    "### trying to draw with networkx\n",
    "# if plot_type == 'spectral':\n",
    "#     plt.figure(figsize=(70,70))\n",
    "#     pos = nx.spring_layout(G, k=0.18, iterations=30)\n",
    "# elif plot_type == 'circular':\n",
    "#     plt.figure(figsize=(12,8))\n",
    "#     pos = nx.circular_layout(G)\n",
    "# else:\n",
    "#     print(\"plot type is not recognized. received \", plot_type)\n",
    "#     exit(-1)\n",
    "# print(\"pair_corr_plot: drawing network\")\n",
    "# nx.draw(G, with_labels=True, pos = pos,node_size=node_size, linewidths=0.5, font_size=15, edge_color=edge_color, width=edge_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in [70,100,150]:\n",
    "# for size in [10]:\n",
    "    # for k in [0.15, 0.18, 0.2, 0.22, 0.25]:\n",
    "    for k in [0.05,0.08,0.1,0.12]:\n",
    "        print(\"pair_corr_plot: making plot/plot_%.3f_in_%dx%d.png\" % (k, size, size))\n",
    "        plt.figure(figsize=(size,size))\n",
    "        pos = nx.spring_layout(G, k=k, iterations=30)\n",
    "        nx.draw(G, with_labels=True, pos = pos,node_size=node_size, linewidths=0.5, font_size=15, edge_color=edge_color, width=edge_width)\n",
    "        plt.savefig(\"plot/plot_%.3f_in_%dx%d.png\" % (k, size, size), dpi=10)\n",
    "        plt.close()\n",
    "        # plt.savefig(\"plot/plot_%.3f_in_%dx%d.png\" % (k, size, size), dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
